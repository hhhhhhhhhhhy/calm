# coding=utf-
"""
Continuous Autoencoder å®ç°

  - Encoder: å°† K ä¸ªç¦»æ•£ token â†’ è¿ç»­å‘é‡  z âˆˆ R^l       ï¼ˆè®ºæ–‡ l=128 æ—¶ K=4ï¼‰
  - Decoder: å°† z è¿˜åŸä¸º K ä¸ª tokenï¼Œé‡å»ºç²¾åº¦ â‰¥99.9 %
  - è®­ç»ƒç›®æ ‡: é‡æ„ CE  +  Î²Â·KL(q_Ï†(z|x)||N(0,I))
"""

from typing import List, Optional, Tuple, Union
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch import nn
from transformers.activations import ACT2FN
from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from transformers.modeling_utils import PreTrainedModel
from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS
from transformers.utils import logging
from .configuration_autoencoder import AutoencoderConfig
from transformers.models.llama.modeling_llama import LlamaPreTrainedModel,LlamaRMSNorm, LlamaMLP

logger = logging.get_logger(__name__)

ALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)

# ------------------------------------------------------------------
# AE Layerï¼šä¸€ä¸ª SwiGLU MLP + Pre-Norm æ®‹å·®
# ------------------------------------------------------------------
class AELayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.mlp = LlamaMLP(config)
        self.layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:

        residual = hidden_states
        hidden_states = self.layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states

# ------------------------------------------------------------------
#      Encoderï¼šè¾“å…¥ x_{1:K} â†’ q_Ï†(z|x)çš„å‚æ•° â†’ é‡‡æ · z
#      è¾“å‡º shape: [B, L, 2*latent_size]  (L=T/K)
#      2* æ˜¯å› ä¸ºè¦åŒæ—¶è¾“å‡º Î¼ å’Œ log Ïƒ
# ------------------------------------------------------------------
class Encoder(LlamaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.patch_size = config.patch_size
        self.latent_size = config.latent_size

        # è¯åµŒå…¥å±‚ |V| -> hidden_size  
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)

        # è‹¥å¹² AE Layerï¼Œåˆ†æˆ 2 ä¸ª stage
        self.encoder_layers = nn.ModuleList([AELayer(config) for _ in range(config.num_encoder_layers)])
        self.num_stage_layers = config.num_encoder_layers // 2

        # çº¿æ€§å±‚ï¼šå°† d ç»´éšè—æ€æ˜ å°„åˆ° 2l ç»´ ï¼šÎ¼æœ‰lç»´, log Ïƒæœ‰lç»´ï¼›æœ€ålç»´åˆ†åˆ«é‡‡æ ·æˆlç»´çš„z
        self.hidden_to_latent = nn.Linear(config.hidden_size, config.latent_size * 2)

        # çº¿æ€§å±‚ï¼šæŠŠ K ä¸ª token çš„æ‹¼æ¥å‹å› d ç»´ï¼ˆstage-0 ç»“æŸåï¼‰
        self.squeeze_layer = nn.Linear(self.patch_size * config.hidden_size, config.hidden_size)
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value
    
    # ---------- å‰å‘ï¼šè¾“å…¥ token id çŸ©é˜µ ---------------------------
    # input_ids: [B, T]  ->  reshape -> [B*L, K]
    # return   : [B, L, 2*l]
    # ---------------------------------------------------------------
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        **kwargs
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        batch_size, seq_length = input_ids.shape
        num_patches = seq_length // self.patch_size    # L = T/K
        input_ids = input_ids.reshape(batch_size * num_patches, self.patch_size)       #æŸ¥è¡¨å¾—åˆ°embedding [B*L, K]

        inputs_embeds = self.embed_tokens(input_ids)    # [B*L, K, d]
        if self.training:
            inputs_embeds = inputs_embeds.to(dtype=torch.bfloat16)

        hidden_states = inputs_embeds     # ä¸‹æ–‡ç®€å†™ h

        # 3. ä¸¤é˜¶æ®µç¼–ç 
        for stage in range(2):
            for layer_idx in range(self.num_stage_layers):
                encoder_idx = stage * self.num_stage_layers + layer_idx
                encoder_layer = self.encoder_layers[encoder_idx]        # ç»´åº¦ä¸å˜
                hidden_states = encoder_layer(hidden_states)    # h (B, hidden_size)

            # stage-0 ç»“æŸåï¼šæŠŠ K ä¸ªå‘é‡æ‹¼èµ·æ¥å‹å› d ç»´
            if stage == 0:
                hidden_states = hidden_states.view(batch_size * num_patches, 1, -1)
                hidden_states = self.squeeze_layer(hidden_states)

        hidden_states = self.norm(hidden_states)        # [B*L, d]

        
        latent_states = self.hidden_to_latent(hidden_states)    # [B*L, 2*l]
        latent_states = latent_states.reshape(batch_size, num_patches, self.latent_size * 2)     # [B, L, 2*l]  

        return latent_states    # Î¼ã€log Ïƒï¼

# ------------------------------------------------------------------
# ğŸ“„ 2.1 Decoderï¼šç»™å®š z â†’ è¿˜åŸ logits  over  K ä¸ª token
#      è¾“å‡º shape: [B, L, K, |V|]
# ------------------------------------------------------------------
class Decoder(LlamaPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.patch_size = config.patch_size
        self.num_stage_layers = config.num_decoder_layers // 2

        # z âˆˆ R^l  â†’  h âˆˆ R^d
        self.latent_to_hidden = nn.Linear(config.latent_size, config.hidden_size)
        self.decoder_layers = nn.ModuleList([AELayer(config) for _ in range(config.num_decoder_layers)])

        self.expand_layer = nn.Linear(config.hidden_size, self.patch_size * config.hidden_size)
        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # Initialize weights and apply final processing
        self.post_init()

    # ---------- å‰å‘ï¼šè¾“å…¥é‡‡æ ·åçš„ z ---------------------------
    # latent_states: [B, L, l]  (å·²é‡å‚æ•°åŒ–)  Læ˜¯patchçš„ä¸ªæ•°
    # return logits: [B, L*K, |V|]
    # ----------------------------------------------------------
    def forward(
        self,
        latent_states,
        **kwargs
    ) -> Union[Tuple, BaseModelOutputWithPast]:

        batch_size, seq_length, latent_size = latent_states.shape   # seq_length = L
        hidden_states = self.latent_to_hidden(latent_states)     # [B, L, l] -> [B, L, d]

        for stage in range(2):
            for layer_idx in range(self.num_stage_layers):
                decoder_idx = stage * self.num_stage_layers + layer_idx
                decoder_layer = self.decoder_layers[decoder_idx]
                hidden_states = decoder_layer(hidden_states)

            if stage == 0:  # ç¬¬ä¸€é˜¶æ®µæ‰§æ€§å®Œçš„æ—¶å€™æ‰©ç»´
                hidden_states = self.expand_layer(hidden_states)    # [B, L, K*d]
                hidden_states = hidden_states.reshape(batch_size, seq_length * self.patch_size, -1)     # [B, L*K, d]

        hidden_states = self.norm(hidden_states)

        logits = F.linear(hidden_states, self.lm_head_weight)       # [B, L*K, |V|]ï¼Œæœ€åä¸€ç»´æ˜¯è¯è¡¨å¤§å°ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªä½ç½®ä¸Šçš„ï¼ˆæœªå½’ä¸€åŒ–ï¼‰çš„æ¦‚ç‡
        # self.lm_head_weightæ˜¯æƒé‡çŸ©é˜µï¼ˆä¸encoderçš„åµŒå…¥çŸ©é˜µç»‘å®šï¼‰

        return logits

# ------------------------------------------------------------------
# å®Œæ•´ VAEï¼šå°è£… Encoder + é‡‡æ · + Decoder + æŸå¤±
#  è®­ç»ƒæ—¶è¿”å› CE + Î²Â·KL
# ------------------------------------------------------------------
class Autoencoder(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config):
        super().__init__(config)
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.patch_size = config.patch_size
        # è®© decoder å¤ç”¨ encoder çš„åµŒå…¥çŸ©é˜µä½œä¸ºè¾“å‡ºå±‚æƒé‡
        self.decoder.lm_head_weight = self.encoder.embed_tokens.weight

        # æ­£åˆ™åŒ–è¶…å‚     
        self.ae_dropout = config.ae_dropout
        self.kl_clamp = config.kl_clamp      # Î»_KLï¼ŒKL clipping é˜ˆå€¼
        self.kl_weight = config.kl_weight    # Î²ï¼ŒKL æƒé‡

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.encoder.embed_tokens

    def set_input_embeddings(self, value):
        self.encoder.embed_tokens = value

    # ---------- å®Œæ•´å‰å‘ï¼šè®­ç»ƒæ¨¡å¼è¿”å› VAE æ€»æŸå¤± ------------------
    # input_ids: [B, T]  (T å¿…é¡»æ˜¯ K çš„å€æ•°)
    # labels   : [B, T]  ä¸ input_ids ç›¸åŒï¼Œç”¨äº CE
    # return   : CausalLMOutputWithPast( loss = CE + Î²Â·KL )
    # ---------------------------------------------------------------
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        labels: Optional[torch.LongTensor] = None,
        **kwargs
    ) -> Union[Tuple, CausalLMOutputWithPast]:

        # 1. è¾“å…¥ dropoutï¼ˆtoken çº§åˆ«ï¼‰
        input_ids = input_ids.reshape(-1, self.patch_size)   # [B*L, K]
        if self.training:
            mask = torch.rand_like(input_ids.float()) > self.ae_dropout
            input_ids = input_ids * mask.long()      # éšæœº mask éƒ¨åˆ† token

        # 2. ç¼–ç å¾— (Î¼, logÏƒ)
        latent_states = self.encoder(input_ids=input_ids)
        mean, log_std = torch.chunk(latent_states, 2, dim=-1)
        std = torch.exp(log_std)
        # é‡‡æ ·å¾—åˆ°  z ~ q_Ï†(z|x)
        eps = torch.randn_like(mean)
        latent_states = mean + eps * std
        
        # 3. latent dropout
        latent_states = torch.nn.functional.dropout(latent_states, p=self.ae_dropout, training=self.training)

        # 4. KL(q||N(0,I))  é€ç»´è®¡ç®—å clamp å†æ±‚å’Œ      
        kl_loss = 0.5 * (torch.pow(mean, 2) + torch.pow(std, 2) - 1 - log_std * 2)
        kl_loss = torch.clamp(kl_loss, min = self.kl_clamp)
        kl_loss = torch.mean(torch.sum(kl_loss, dim=-1))

        # 5. è§£ç  & CE
        logits = self.decoder(latent_states=latent_states).float()      # [B*L*K, |V|] ï¼ŒL*Kæ˜¯ tokenæ•°ï¼šKä¸ªä¸€ç»„ï¼Œå…±Lç»„
        loss_fct = nn.CrossEntropyLoss()
        logits = logits.view(-1, self.config.vocab_size)
        labels = labels.view(-1).to(logits.device)
        loss = loss_fct(logits, labels) 

        # æ€»çš„lossï¼šCE + KLæ•£åº¦ï¼ŒKLæ˜¯ä¸ºäº†çº¦æŸzçš„åˆ†å¸ƒä½¿å…¶å¯¹é½æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œåˆ†å¸ƒæ›´å¹³æ»‘ï¼Œé˜²æ­¢å¾®å°æ‰°åŠ¨
        if self.training:
            loss = loss * self.patch_size + kl_loss * self.kl_weight

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits
        )

